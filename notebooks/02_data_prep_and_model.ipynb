{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 1: imports & basic config\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cda96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section 2: EDA-driven config (rules decided in 01_eda)\n",
    "\n",
    "DATA_PATH = \"../data/raw/telecom_churn_dataset.csv\"\n",
    "\n",
    "TARGET_COL = \"CHURN\" \n",
    "TARGET_FLAG_COL = \"churn_flag\"\n",
    "\n",
    "# 1) Features to exclude\n",
    "EXCLUDE_FEATURES = [\n",
    "    \"PID\",                    # pure ID\n",
    "    \"Suspended_subscribers\",  # ~96% missing\n",
    "    \"Not_Active_subscribers\", # ~50% missing\n",
    "    \"Billing_ZIP\",            # excluded in v1, maybe revisit later\n",
    "]\n",
    "\n",
    "# 2) Columns where missing values are not acceptable → drop those rows\n",
    "ROW_DROP_MISSING_COLS = [\n",
    "    TARGET_COL,               # drop rows with missing CHURN\n",
    "    \"CRM_PID_Value_Segment\",  # 5 rows missing → drop\n",
    "    \"ARPU\",                   # 1 row missing → drop\n",
    "]\n",
    "\n",
    "# 3) Revenue columns used for feature engineering\n",
    "AVG_MOBILE_COL = \"Average Mobile Revenue\"\n",
    "AVG_FIX_COL    = \"Average Fix Revenue\"\n",
    "TOTAL_REV_COL  = \"Total Revenue\"\n",
    "\n",
    "# engineered feature names\n",
    "MOBILE_SHARE_COL = \"mobile_share\"\n",
    "FIXED_SHARE_COL  = \"fixed_share\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15229f",
   "metadata": {},
   "source": [
    "load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(path: str = DATA_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load the raw dataset from CSV.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"[load_raw_data] Loaded shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56003d66",
   "metadata": {},
   "source": [
    "row-level cleanup (drop bad rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply row-level cleaning:\n",
    "    - Drop rows with missing values in critical columns (CHURN, CRM_PID_Value_Segment, ARPU).\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    print(f\"[clean_rows] Starting shape: {df_clean.shape}\")\n",
    "    \n",
    "    for col in ROW_DROP_MISSING_COLS:\n",
    "        if col in df_clean.columns:\n",
    "            n_before = df_clean.shape[0]\n",
    "            n_missing = df_clean[col].isna().sum()\n",
    "            if n_missing > 0:\n",
    "                df_clean = df_clean.dropna(subset=[col])\n",
    "                n_after = df_clean.shape[0]\n",
    "                print(\n",
    "                    f\"[clean_rows] Dropped {n_before - n_after} rows due to missing values in '{col}' \"\n",
    "                    f\"({n_missing} missing). New shape: {df_clean.shape}\"\n",
    "                )\n",
    "        else:\n",
    "            print(f\"[clean_rows] WARNING: column '{col}' not found in dataframe.\")\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628e002",
   "metadata": {},
   "source": [
    "feature engineering: revenue shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_revenue_share_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create mobile_share and fixed_share features using revenue columns.\n",
    "    After creating them, drop 'Total Revenue' from the feature space.\n",
    "    \"\"\"\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # check columns exist\n",
    "    needed_cols = [AVG_MOBILE_COL, AVG_FIX_COL, TOTAL_REV_COL]\n",
    "    missing_needed = [c for c in needed_cols if c not in df_fe.columns]\n",
    "    if missing_needed:\n",
    "        raise ValueError(f\"[add_revenue_share_features] Missing required columns: {missing_needed}\")\n",
    "    \n",
    "    total_rev = df_fe[TOTAL_REV_COL].replace({0: np.nan})  # avoid division by zero\n",
    "    \n",
    "    df_fe[MOBILE_SHARE_COL] = df_fe[AVG_MOBILE_COL] / total_rev\n",
    "    df_fe[FIXED_SHARE_COL]  = df_fe[AVG_FIX_COL] / total_rev\n",
    "    \n",
    "    # You could also decide to fill NaNs with 0 here if that makes business sense.\n",
    "    \n",
    "    # Drop Total Revenue from modeling features (kept only in df_fe if needed elsewhere)\n",
    "    df_fe = df_fe.drop(columns=[TOTAL_REV_COL])\n",
    "    \n",
    "    print(\"[add_revenue_share_features] Added columns:\",\n",
    "          MOBILE_SHARE_COL, FIXED_SHARE_COL,\n",
    "          \"and dropped:\", TOTAL_REV_COL)\n",
    "    \n",
    "    return df_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc2eeb",
   "metadata": {},
   "source": [
    "build X and y (column-level cleaning + target encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5030f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xy(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    From a cleaned dataframe:\n",
    "    - Encode target (CHURN -> churn_flag).\n",
    "    - Apply revenue share feature engineering.\n",
    "    - Drop excluded columns from the feature set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame  (features)\n",
    "    y : pd.Series      (binary target)\n",
    "    \"\"\"\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    # 1) target encoding\n",
    "    if TARGET_COL not in df_proc.columns:\n",
    "        raise ValueError(f\"[build_xy] Target column '{TARGET_COL}' not found.\")\n",
    "    \n",
    "    df_proc[TARGET_FLAG_COL] = df_proc[TARGET_COL].map({\"Yes\": 1, \"No\": 0})\n",
    "    if df_proc[TARGET_FLAG_COL].isna().any():\n",
    "        raise ValueError(\"[build_xy] Unexpected values in CHURN, mapping produced NaNs.\")\n",
    "    \n",
    "    # 2) feature engineering: revenue mix\n",
    "    df_proc = add_revenue_share_features(df_proc)\n",
    "    \n",
    "    # 3) columns to drop from X\n",
    "    drop_cols = [TARGET_COL, TARGET_FLAG_COL] + EXCLUDE_FEATURES\n",
    "    drop_cols = [c for c in drop_cols if c in df_proc.columns]  # guard for missing columns\n",
    "    \n",
    "    X = df_proc.drop(columns=drop_cols)\n",
    "    y = df_proc[TARGET_FLAG_COL]\n",
    "    \n",
    "    print(f\"[build_xy] X shape: {X.shape}, y length: {len(y)}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa2bd2",
   "metadata": {},
   "source": [
    "preprocessor (numeric + categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Build preprocessing pipeline without imputation:\n",
    "    - Numeric: StandardScaler only\n",
    "    - Categorical: OneHotEncoder only\n",
    "\n",
    "    Assumes all missing values have already been removed earlier.\n",
    "    Raises an error if any NaNs are still present in X.\n",
    "    \"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"[build_preprocessor] Numeric cols: {len(numeric_cols)}, \"\n",
    "          f\"Categorical cols: {len(categorical_cols)}\")\n",
    "\n",
    "    # --- Strict safety check: no NaNs allowed anywhere ---\n",
    "    has_na_num = X[numeric_cols].isna().any().any() if numeric_cols else False\n",
    "    has_na_cat = X[categorical_cols].isna().any().any() if categorical_cols else False\n",
    "\n",
    "    if has_na_num or has_na_cat:\n",
    "        raise ValueError(\n",
    "            \"[build_preprocessor] Found missing values in features but \"\n",
    "            \"imputation is disabled. Clean/drop all NaNs earlier \"\n",
    "            \"before calling build_preprocessor().\"\n",
    "        )\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a70e7",
   "metadata": {},
   "source": [
    "model + training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df936185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Return an untrained classifier model.\"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_pipeline(X: pd.DataFrame, y: pd.Series):\n",
    "    \"\"\"\n",
    "    Compose preprocessor + model into a single sklearn Pipeline and fit it.\n",
    "    \"\"\"\n",
    "    preprocessor = build_preprocessor(X)\n",
    "    model = build_model()\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X, y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ee00",
   "metadata": {},
   "source": [
    "run end-to-end in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section N: main training flow\n",
    "\n",
    "# 1. Load & clean\n",
    "df_raw = load_raw_data()\n",
    "df_clean = clean_rows(df_raw)\n",
    "\n",
    "# 2. Build X and y using our EDA-driven data prep\n",
    "X, y = build_xy(df_clean)\n",
    "\n",
    "# 3. Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"[main] Train shape: {X_train.shape}, Val shape: {X_val.shape}\")\n",
    "\n",
    "# 4. Train pipeline\n",
    "pipeline = train_pipeline(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate\n",
    "# --- Probabilities for ROC AUC ---\n",
    "y_val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "roc = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"[main] Validation ROC AUC: {roc:.3f}\")\n",
    "\n",
    "# --- Convert to hard labels with threshold 0.5 ---\n",
    "threshold = 0.5\n",
    "y_val_pred = (y_val_proba >= threshold).astype(int)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"[main] Confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "# --- Core metrics for positive class (churn=1) ---\n",
    "precision = precision_score(y_val, y_val_pred, pos_label=1)\n",
    "recall    = recall_score(y_val, y_val_pred,    pos_label=1)\n",
    "f1        = f1_score(y_val, y_val_pred,       pos_label=1)\n",
    "\n",
    "print(f\"[main] Precision (churn=1): {precision:.3f}\")\n",
    "print(f\"[main] Recall    (churn=1): {recall:.3f}\")\n",
    "print(f\"[main] F1        (churn=1): {f1:.3f}\")\n",
    "\n",
    "# --- Full breakdown per class (0 and 1) ---\n",
    "print(\"\\n[main] Classification report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
